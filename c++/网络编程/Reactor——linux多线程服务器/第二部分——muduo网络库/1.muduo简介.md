## 一、简介

### 1.目录结构：

![image-20211228225146489](https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/20211228225202.png)

- muduo/bash都是用户可见的基础库：

![image-20211228225700108](https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/20211228225701.png)

- 网络核心库

muduo是基于reactor模式的网络库，核心是事件循环EventLoop，用于响应计时器和IO事件。采用基于对象而不是面向对象的设计风格，其事件回调接口多以boost：：function+boost::bind表达，使用时不需要继承class。

网络库核心位于muduo/net、muduo/net/poller，4k多行代码：

![image-20211228230157269](https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/20211228230159.png)

- 附属网络库

  位于muduo/net/{http,inspect,protorpc}等处

  ![image-20211228230351219](https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/20211228230353.png)

### 2.代码结构

mudo头文件分为客户可见与不可见。以下是安装后暴露的头文件和库文件。

![image-20211228230507892](https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/20211228230509.png)

![image-20211228230619940](https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/20211228230626.png)

muduo的使用只需要掌握5个关键类：buffer、eventloop、tcpconnection、tcpclient、tcpserver。

头文件包含关系如下：

![image-20211228230702280](https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/20211228230703.png)

muduo头文件使用了向前申明（？），简化头文件依赖关系。

- 如Acceptor、Channel、Connector、Tcpconnection都向前申明了 EventLoop class，从而避免包含EventLoop.h。

#### 1.公开接口

- Buffer，仿Netty ChannelBuffer的buffer class，数据读写通过buffer进行。用户只需要处理接收到的数据和准备要发送的数据。

- InetAddress封装IPv4地址（end poiont），不能解析域名，只认ip地址。因为直接使用getthostbyname（3）解析域名会阻塞IO线程。
- EventLoop事件循环（Reactor）,每个线程只能有一个EventLoop实体，负责IO和定时器事件的分派。它用eventfd（2）来异步唤醒，区别与用一对pipe的方法。用TimerQueue作为计时器管理，Poller作为IO multiplexing。
- EventLoopThread启动一个线程，其中运行EventLoop：：loop（）
- TcpConnection整个网络库的核心，封装一次TCP连接，不能发起连接。
- TcpClient用于编写网络客户端，发起连接，有重试功能。
- TcpServer用于编写网络服务器，接收连接。

TcpConnection的生命周期依靠share_ptr管理，Buffer的生命期由TcpConnection控制。其余类的生命周期由用户控制。Buffer、InetAddress具有值意义，可以拷贝；其他class都是对象语义，不能拷贝。

#### 2.内部实现

- Channel是selectable IO Channel，负责注册与响应IO事件，不拥有file descripptor，是Acceptor、Connector、EventLoop、TcpConnection的成员，生命期有后者控制。
- Socket是一个RAII handle，封装一个file descriptor，在析构时关闭fd。是Acceptor、TcpConnection的成员。EventLoop、TimerQueue也有fd，但是不封装为socket class。
- SocketsOps封装各种Sockets系统调用
- Poller是PollPoller和EpollPoller的基类，是EventLoop成员。
- PollPoller和EPollPoller封装poll和epoll两种IO multiplexing后端。poll的存在便于调试，因为poll调用是上下文无关的，用strace可以知道库行为是否正确。
- Connector用于发起TVP连接，是TcpClient成员
- Acceptor用于接收Tcp连接，是TcpServer成员
- TimerQueue用timerfd实现定时，区别于传统的poll/epoll_wait的等待时长方法。TimerQueue用std：：map管理Timer，常用操作复杂度是O（log n）。n为定时器数量。是EventLoop成员。
- EventLoopThreadPool用于创建IO线程池，把TcpConnection分派到某个EventLoop线程上，是TcpServer成员。

![image-20211228233322908](https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/20211228233324.png)

### 3.例子

![image-20211228233634377](https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/20211228233636.png)

![image-20211228233650352](https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/20211228233651.png)

## 二、线程模型

one loop per thread + thread pool模型。每个线程最多有一个EventLoop，每个TcpConnetion必须归某个EventLoop管理，所有IO会转移到这个线程。这样可以方便的将不同TCP放到不同线程，一些TCP连接放入一个线程。TcpConnetion、EventLoop是线程安全的，可以跨线程调用。

TcpServer支持多线程，有两种模式：

- 单线程，accept与TcpConnection用同一个线程做IO
- 多线程，accept与EventLoop在同一线程，新创建一个EventLoopThreadPool，新连接会按round-robin方式分配到线程池中。

muduo只支持linux2.6.x下的并发非阻塞TCP网络编程，核心是每个IO线程一个事件循环，把IO分发带回调函数。



## 三、TCP网络编程本质

基于事件的非阻塞网络编程是编写高性能并发服务器的主流模式，将原来住的调用recv、accept、send函数变为注册回调函数，网络库收到数据后调用回调函数，把数据提供给回调函数的方式。类似于win32的消息循环，消息循环中的代码避免阻塞失去响应。

### 1. 三个半事件

作者认为TCP网络编程最本质是处理三个半事件：

- 连接的建立，包括服务端accept和客户端的connect。Tcp连接建立后，client与server地位平等。
- 连接的断开，包括主动断开（close、shutdown）与被动端口（read返回0）。
- 消息到达，文件描述符刻度。最重要的事件，其处理方式决定网络编程风格（阻塞or非阻塞，如何处理分包，应用层缓冲设计等）
- （半）消息发送完毕。对于低流量分为，可以不必关心这个事件；发送完毕是指将数据写入操作系统缓冲区，由TCP协议负责数据发送与重传，不表示对方已收到数据。

## 四、常见的并发网络服务设计方案

<img src="https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/20211229233754.png" alt="image-20211229225201550" style="zoom:150%;" />

其中，“互通”指的是chat 服务，多个客户连接之间是否能方便交换数据；“顺序性”指的是在httpd/SudoKu这类响应服务中，客户连接顺序发送多个请求，那么计算结果是否按相同顺序返回客户（自然返回）。

UNP CSDA主要是阻塞式网络服务，方案0-4归此。方案5是用的很多的单线程Reactor方案，muduo支持。方案6，7不算实用方案。方案8，9是书重点介绍方案。

- 方案0：是iterative服务器，它一次只能服务一个客户。不适合长连接，适合短连接服务。

- 方案1：传统unix并发网络编程方案，clid-per-client或者fork-per-client，或process-per-connection。适合并发数不多的情况，适用于计算响应的工作量远大于fork的开销。适合长连接。

- 方案2：传统的java网络编程方案，thread-per-connection。不适合短连接服务，伸缩性受到线程数限制。

- 方案3：针对方案1优化

- 方案4：针对方案2优化

- 方案5：由网络库搞定数据收发，程序只关心业务。适用IO密集型应用，不适合CPU密集型应用。与方案2相比，成功优酷网络信息延迟稍大，因为方案2只read一次就可以获取数据，而5需要先poll再read。使用非阻塞IO+事件驱动方式编程时，避免时间回调中指向耗时操作，包括IO等。

- 方案6：过渡方案。线程开销大，每个请求创建新线程。有out-of-order特点，同时创建多个线程区计算同一个连接上的请求，算出的结果不确定。

- 方案7：为了让计算结果顺序确定，为每个连接创建一个计算线程，每个连接上的请求固定发给同一线程计算。过渡方案。受限于线程数。与方案6区别是，方案6一个TCP连接发一长串突发请求可以占满8个core；而方案7最多占用12.5%，因为都是同一线程计算。

- 方案8：弥补方案6线程创建消耗大的缺陷，使用固定线程池。全部IO工作都在一个Reactor线程中，计算任务交给线程池。如果计算任务独立，IO压力不大，这种方案非常适用。

  ![image-20211229231756153](https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/20211229231758.png)

  方案8有乱序返回的可能，客户端需要根据id匹配响应。

- 方案9：muduo内置多线程方案，也是netty内置多线程方案。特点是one loop per thread，有个main Reactor负责accept连接，然后把连接挂在某个sub Reactor中，这样改连接的所有操作都在那个sub Reactor的线程中。多个连接可能被分到多个线程中，充分利用CPU。

  muduo采用固定大小的Reactor pool，大小根据CPU数量确定。好处是总体的处理能力不会随连接数量增加而下降。由于一个连接由一个线程管理，请求顺序性有保证，突发请求不会占满8个core。

  相比于方案8，9减少了进出线程池的两次上下文切换。

- 方案10：nginx内置方案，如果连接之间无交互，这个比较合适，工作进程互相独立，可以热升级。

- 方案11：混合方案8/9，既可以使用多个Reactor处理IO，也可以使用线程池计算。适合既有突发IO，又有突发计算的应用。

![image-20211229232739152](https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/20211229232741.png)

实用方案有5种，muduo支持后4种：![image-20211229233258023](https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/20211229233259.png)

C1/C2是与连接数无关、CPU有关的常数。



### 1.程序到底使用一个EventLoop合适多个event Loop呢？

zeroMQ的建议是：按照每千兆比特每秒的吞吐量配一个event loop的比例设置数目，及muduo::TcpServer::setThreadNum的参数。

以上假定TCP连接没有优先级，所以看重吞吐量。如果有优先级，把高优先级的连接单独用event loop处理。

在muduo中，同一个event loop中的连接之间没有事件优先级差别，设计目的是防止优先级反转。


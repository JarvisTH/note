## 一、网络爬虫

网络爬虫又称网络蜘蛛、网络机器人，它是一种按照一定的规则自动浏览、检索网页信息的程序或者脚本。网络爬虫能够自动请求网页，并将所需要的数据抓取下来。通过对抓取的数据进行处理，从而提取出有价值的信息。

### 1.爬虫分类

**通用网络爬虫**：是搜索引擎的重要组成部分，上面已经进行了介绍，这里就不再赘述。通用网络爬虫需要遵守 robots 协议，网站通过此协议告诉搜索引擎哪些页面可以抓取，哪些页面不允许抓取。

**聚焦网络爬虫**：是面向特定需求的一种网络爬虫程序。它与通用爬虫的区别在于，聚焦爬虫在实施网页抓取的时候会对网页内容进行筛选和处理，尽量保证只抓取与需求相关的网页信息。聚焦网络爬虫极大地节省了硬件和网络资源，由于保存的页面数量少所以更新速度很快，这也很好地满足一些特定人群对特定领域信息的需求。

**增量式网络爬虫**：是指对已下载网页采取增量式更新，它是一种只爬取新产生的或者已经发生变化网页的爬虫程序，能够在一定程度上保证所爬取的页面是最新的页面。

## 二、静态网页和动态网页

在编写一个爬虫程序前，首先要明确待爬取的页面是静态的，还是动态的，只有确定了页面类型，才方便后续对网页进行分析和程序编写。对于不同的网页类型，编写爬虫程序时所使用的方法也不尽相同。

### 1.静态网页

静态网页是标准的 HTML 文件，通过 GET 请求方法可以直接获取，文件的扩展名是`.html`、`.htm`等，网面中可以包含文本、图像、声音、FLASH 动画、客户端脚本和其他插件程序等。静态网页是网站建设的基础，早期的网站一般都是由静态网页制作的。

静态网页的数据全部包含在 HTML 中，因此爬虫程序可以直接在 HTML 中提取数据。通过分析静态网页的 URL，并找到 URL 查询参数的变化规律，就可以实现页面抓取。与动态网页相比，并且静态网页对搜索引擎更加友好，有利于搜索引擎收录。

### 2.动态网页

动态网页指的是采用了动态网页技术的页面，比如 AJAX、ASP、JSP整个页面内容，就可以实现网页的局部更新。动态页面使用“动态页面技术”与服务器进行少量的数据交换，从而实现了网页的异步加载。

抓取动态网页的过程较为复杂，需要通过动态抓包来获取客户端与服务器交互的 JSON 数据。抓包时，可以使用谷歌浏览器开发者模式（快捷键：F12）`Network`选项，然后点击 XHR，找到获取 JSON 数据的 URL，如下所示：

![image-20230306224239709](https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/image-20230306224239709.png)

### 3.审查网页元素

要善于发现网页元素的规律，并且能从中提炼出有效的信息。因此，在动手编写爬虫程序前，必须要对网页元素进行审查。

## 三、User-Agent（用户代理）

User-Agent 即用户代理，简称“UA”，它是一个特殊字符串头。网站服务器通过识别 “UA”来确定用户所使用的操作系统版本、CPU 类型、浏览器版本等信息。而网站服务器则通过判断 UA 来给客户端发送不同的页面。

User-Agent 就是反爬策略的第一步。

网站通过识别请求头中 User-Agent 信息来判断是否是爬虫访问网站。如果是，网站首先对该 IP 进行预警，对其进行重点监控，当发现该 IP 超过规定时间内的访问次数， 将在一段时间内禁止其再次访问网站。

常见的 User-Agent 请求头，如下所示：

| 系统    | 浏览器  | User-Agent字符串                                             |
| ------- | ------- | ------------------------------------------------------------ |
| Mac     | Chrome  | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36 |
| Mac     | Firefox | Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:65.0) Gecko/20100101 Firefox/65.0 |
| Mac     | Safari  | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0.3 Safari/605.1.15 |
| Windows | Edge    | Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/18.17763 |
| Windows | IE      | Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko |
| Windows | Chrome  | Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 |
| iOS     | Chrome  | Mozilla/5.0 (iPhone; CPU iPhone OS 7_0_4 like Mac OS X) AppleWebKit/537.51.1 (KHTML, like Gecko) CriOS/31.0.1650.18 Mobile/11B554a Safari/8536.25 |
| iOS     | Safari  | Mozilla/5.0 (iPhone; CPU iPhone OS 8_3 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12F70 Safari/600.1.4 |
| Android | Chrome  | Mozilla/5.0 (Linux; Android 4.2.1; M040 Build/JOP40D) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.59 Mobile Safari/537.36 |
| Android | Webkit  | Mozilla/5.0 (Linux; U; Android 4.4.4; zh-cn; M351 Build/KTU84P) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30 |

### 1.爬虫程序UA信息

![image-20230306231614066](https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/image-20230306231614066.png)

### 2.重构爬虫UA信息

![image-20230306231712051](https://cdn.jsdelivr.net/gh/JarvisTH/picbed/img/image-20230306231712051.png)

### 3.User-Agnet代理池

在编写爬虫程序时，一般都会构建一个 User-Agent （用户代理）池，就是把多个浏览器的 UA 信息放进列表中，然后再从中随机选择。构建用户代理池，能够避免总是使用一个 UA 来访问网站，因为短时间内总使用一个 UA 高频率访问的网站，可能会引起网站的警觉，从而封杀掉 IP。

- ## 自定义UA代理池

工作目录中定义一个 ua_info.py 文件，并将以下 UA 信息以列表的形式粘贴到该文件中，如下所示：

```python
ua_list = [
    'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11',
    'User-Agent:Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11',
    'Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1',
    'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)',
    'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50',
    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0',
    ' Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1',
    'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1',
    ' Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1',
]
```

- ## 模块随机获取UA

使用专门第三方的模块来随机获取浏览器 UA 信息，不过该模块需要单独安装，安装方式如下：

```
pip install fake-useragent
```

```python
from fake_useragent import UserAgent
#实例化一个对象
ua=UserAgent()
#随机获取一个ie浏览器ua
print(ua.ie)
print(ua.ie)
#随机获取一个火狐浏览器ua
print(ua.firefox)
print(ua.firefox)

#随机获取ie的ua信息
Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/4.0; GTB7.4; InfoPath.3; SV1; .NET CLR 3.1.76908; WOW64; en-US)
Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0
	
#随机获取火狐的ua信息
Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; rv:21.0) Gecko/20100101 Firefox/21.0
Mozilla/5.0 (Windows NT 5.0; rv:21.0) Gecko/20100101 Firefox/21.0
```

## 四、URL编码/解码详解

当 URL 路径或者查询参数中，带有中文或者特殊字符的时候，就需要对 URL 进行编码（采用十六进制编码格式）。URL 编码的原则是使用安全字符去表示那些不安全的字符。

安全字符，指的是没有特殊用途或者特殊意义的字符。

### 1.URL基本组成

```
http://www.biancheng.net/index?param=10
```

路径和查询字符串之间使用问号`?`隔开。上述示例的域名为 www.biancheng.net，路径为 index，查询字符串为 param=1。

URL 中规定了一些具有特殊意义的字符，常被用来分隔两个不同的 URL 组件，这些字符被称为**保留字符**。例如：

- 冒号：用于分隔协议和主机组件，斜杠用于分隔主机和路径
- `?`：用于分隔路径和查询参数等。
- `=`用于表示查询参数中的键值对。
- `&`符号用于分隔查询多个键值对。

```
其余常用的保留字符有：/ . ... # @ $ + ; %
```

### 2.哪些字符需要编码

URL 之所以需要编码，是因为 URL 中的某些字符会引起歧义，比如 URL 查询参数中包含了”&”或者”%”就会造成服务器解析错误；再比如，URL 的编码格式采用的是 ASCII 码而非 Unicode 格式，这表明 URL 中不允许包含任何非 ASCII 字符（比如中文），否则就会造成 URL 解析错误。

URL 编码协议规定（RFC3986 协议）：URL 中只允许使用 ASCII 字符集可以显示的字符，比如英文字母、数字、和`- _ . ~ ! *`这 6 个**特殊字符**。当在 URL 中使用不属于 ASCII 字符集的字符时，就要使用特殊的符号对该字符进行编码，比如空格需要用`%20`来表示。

除了无法显示的字符需要编码外，还需要对 URL 中的部分**保留字符**和**不安全字符**进行编码。下面列举了部分不安全字符：

```
[ ] < > " "  { } | \ ^ * · ‘ ’ 等
```

URL特殊字符编码：

| 字符 | 含义                               | 十六进制值编码 |
| ---- | ---------------------------------- | -------------- |
| +    | URL 中 + 号表示空格                | %2B            |
| 空格 | URL中的空格可以编码为 + 号或者 %20 | %20            |
| /    | 分隔目录和子目录                   | %2F            |
| ?    | 分隔实际的 URL 和参数              | %3F            |
| %    | 指定特殊字符                       | %25            |
| #    | 表示书签                           | %23            |
| &    | URL 中指定的参数间的分隔符         | %26            |
| =    | URL 中指定参数的值                 | %3D            |

下面简单总结一下，哪些字符需要编码，分为以下三种情况：

- ASCII 表中没有对应的可显示字符，例如，汉字。
- 不安全字符，包括：# ”% <> [] {} | \ ^ ` 。
- 部分保留字符，即 & / : ; = ? @ 。